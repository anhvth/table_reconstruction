{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_bounding/049.png\n",
      "data/train_bounding/000.png\n",
      "data/train_bounding/132.png\n",
      "data/train_bounding/017.png\n",
      "data/train_bounding/023.png\n",
      "data/train_bounding/031.png\n",
      "data/train_bounding/127.png\n",
      "data/train_bounding/030.png\n",
      "data/train_bounding/037.png\n",
      "data/train_bounding/015.png\n",
      "data/train_bounding/022.png\n",
      "data/train_bounding/028.png\n",
      "data/train_bounding/038.png\n",
      "data/train_bounding/010.png\n",
      "data/train_bounding/041.png\n",
      "data/train_bounding/014.png\n",
      "data/train_bounding/018.png\n",
      "data/train_bounding/032.png\n",
      "data/train_bounding/051.png\n",
      "data/train_bounding/008.png\n",
      "data/train_bounding/130.png\n",
      "data/train_bounding/126.png\n",
      "data/train_bounding/043.png\n",
      "data/train_bounding/034.png\n",
      "data/train_bounding/024.png\n",
      "data/train_bounding/019.png\n",
      "data/train_bounding/096.png\n",
      "data/train_bounding/003.png\n",
      "data/train_bounding/001.png\n",
      "data/train_bounding/128.png\n",
      "data/train_bounding/046.png\n",
      "data/train_bounding/012.png\n",
      "data/train_bounding/134.png\n",
      "data/train_bounding/045.png\n",
      "data/train_bounding/029.png\n",
      "data/train_bounding/025.png\n",
      "data/train_bounding/131.png\n",
      "data/train_bounding/040.png\n",
      "data/train_bounding/044.png\n",
      "data/train_bounding/144.png\n",
      "data/train_bounding/050.png\n",
      "data/train_bounding/047.png\n",
      "data/train_bounding/013.png\n",
      "data/train_bounding/009.png\n",
      "data/train_bounding/033.png\n",
      "data/train_bounding/133.png\n",
      "data/train_bounding/011.png\n",
      "data/train_bounding/026.png\n",
      "data/train_bounding/006.png\n",
      "data/train_bounding/129.png\n",
      "data/train_bounding/016.png\n",
      "data/train_bounding/042.png\n",
      "data/train_bounding/004.png\n",
      "data/train_bounding/053.png\n",
      "data/train_bounding/007.png\n",
      "data/train_bounding/039.png\n",
      "data/train_bounding/027.png\n",
      "data/train_bounding/052.png\n",
      "data/train_bounding/002.png\n",
      "data/train_bounding/097.png\n",
      "data/train_bounding/020.png\n",
      "data/train_bounding/048.png\n",
      "data/train_bounding/021.png\n",
      "data/train_bounding/005.png\n"
     ]
    }
   ],
   "source": [
    "!python tools/create_train_data_from_label_xml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Counting objects: 4, done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (4/4), done.\n",
      "From https://github.com/vnbot2/thesis\n",
      "   d0ecbcb..bde2657  master     -> origin/master\n",
      "Updating d0ecbcb..bde2657\n",
      "Fast-forward\n",
      " tools/create_data.py | 3 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n",
      " 1 file changed, 1 insertion(+), 2 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "len(glob('data/synthetic_data_bounding/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__MACOSX\t\t text_line_verified\t train_bounding\r\n",
      "synthetic_data_bounding  text_line_verified.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128x1024_64_data\r\n"
     ]
    }
   ],
   "source": [
    "! ls pix2pix/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded lab_colorization = False\n",
      "loaded ndf = 32\n",
      "loaded ngf = 32\n",
      "loaded which_direction = AtoB\n",
      "aspect_ratio = 1.0\n",
      "batch_size = 1\n",
      "beta1 = 0.5\n",
      "checkpoint = pix2pix/output/128x1024_64_data\n",
      "display_freq = 0\n",
      "flip = False\n",
      "gan_weight = 1.0\n",
      "input_dir = None\n",
      "l1_weight = 100.0\n",
      "lab_colorization = False\n",
      "lr = 0.0002\n",
      "max_epochs = None\n",
      "max_steps = None\n",
      "mode = export\n",
      "ndf = 32\n",
      "ngf = 32\n",
      "output_dir = pix2pix/frozen/128x1024\n",
      "output_filetype = png\n",
      "progress_freq = 50\n",
      "save_freq = 1000\n",
      "scale_size = [128, 1024]\n",
      "seed = 2139197480\n",
      "separable_conv = False\n",
      "summary_freq = 100\n",
      "trace_freq = 0\n",
      "which_direction = AtoB\n",
      "Batch input: Tensor(\"truediv:0\", shape=(?, 128, 1024, 3), dtype=float32)\n",
      "Create GENERATOR--------------------------------\n",
      "(?, 64, 512, 32)\n",
      "(?, 32, 256, 64)\n",
      "(?, 16, 128, 128)\n",
      "(?, 8, 64, 256)\n",
      "(?, 4, 32, 256)\n",
      "(?, 2, 16, 256)\n",
      "(?, 1, 8, 256)\n",
      "(?, 1, 4, 256)\n",
      "DECONV\n",
      "(?, 1, 8, 256)\n",
      "(?, 2, 16, 256)\n",
      "(?, 4, 32, 256)\n",
      "(?, 8, 64, 256)\n",
      "(?, 16, 128, 128)\n",
      "(?, 32, 256, 64)\n",
      "(?, 64, 512, 32)\n",
      "(?, 128, 1024, 3)\n",
      "image: Tensor(\"generator/decoder_1/Tanh:0\", shape=(?, 128, 1024, 3), dtype=float32)\n",
      "2018-03-31 00:24:24.162223: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-03-31 00:24:24.460429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-03-31 00:24:24.460794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 7.42GiB\n",
      "2018-03-31 00:24:24.460813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "loading model from checkpoint\n",
      "exporting model: pix2pix/output/128x1024_64_data/model-150000\n"
     ]
    }
   ],
   "source": [
    "!bash script/pix2pix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta path: pix2pix/output/frozen/128x1024_v3//export.meta\n",
      "INFO:tensorflow:Restoring parameters from pix2pix/output/frozen/128x1024_v3/export\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "class argument:\n",
    "    def __init__(self):\n",
    "        self.strides = [64, 256]\n",
    "        self.k_size =[128,512]\n",
    "        self.checkpoint = 'pix2pix/output/frozen/128x1024_v3/'\n",
    "        self.input_dir = 'data/ano'\n",
    "        self.output_dir = 'output/run_method2'\n",
    "args = argument()\n",
    "meta_path = '{}/export.meta'.format(args.checkpoint)\n",
    "print('meta path:', meta_path)\n",
    "assert os.path.exists(meta_path)\n",
    "tf.reset_default_graph()\n",
    "tf.train.import_meta_graph(meta_path)\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(sess, tf.train.latest_checkpoint(\n",
    "    args.checkpoint))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_tensor_by_name(name):\n",
    "    name_on_device = '{}:0'.format(name)\n",
    "    return tf.get_default_graph().get_tensor_by_name(name_on_device)\n",
    "\n",
    "\n",
    "def resize(image):\n",
    "    h, w = image.shape[:2]\n",
    "    new_h = math.ceil(h/args.strides[0])*args.strides[0]\n",
    "    new_w = math.ceil(w/args.strides[1])*args.strides[1]\n",
    "    return cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "def extract_patches(image, k_size, strides):\n",
    "    images = tf.extract_image_patches(tf.expand_dims(\n",
    "        image, 0), k_size, strides, rates=[1, 1, 1, 1], padding='SAME')[0]\n",
    "    images_shape = tf.shape(images)\n",
    "    images_reshape = tf.reshape(\n",
    "        images, [images_shape[0]*images_shape[1], *k_size[1:3], 3])\n",
    "    images, n1, n2 = tf.cast(images_reshape, tf.uint8) , images_shape[0], images_shape[1]\n",
    "    return images, n1, n2\n",
    "\n",
    "def join_patches(images, n1, n2, k_size, strides):\n",
    "\n",
    "    s1 = k_size[1]//2-strides[1]//2\n",
    "    s2 = k_size[2]//2-strides[2]//2\n",
    "    roi = images[:, \n",
    "                 s1:s1+strides[1],\\\n",
    "                 s2:s2+strides[2],\n",
    "                 :]\n",
    "    new_shape = [n1, n2, *roi.shape[1:]]\n",
    "    reshaped_roi = tf.reshape(roi, new_shape)\n",
    "    reshaped_roi = tf.transpose(reshaped_roi, perm=[0,2,1,3,4])\n",
    "    rs = tf.shape(reshaped_roi)\n",
    "    rv = tf.cast(tf.reshape(reshaped_roi, [rs[0]*rs[1], rs[2]*rs[3], -1]), tf.uint8)\n",
    "    return rv\n",
    "\n",
    "def run_image(image):\n",
    "    h, w = image.shape[:2]\n",
    "    resized_image = resize(image)\n",
    "    splited_images, n1, n2 = extract_patches(image, [1, 128, 512,1], [1,64,256,1])\n",
    "    print('n1,n2:',sess.run([n1, n2]))\n",
    "    splited_images_ = sess.run(splited_images)\n",
    "    print(type(splited_images_), splited_images_.shape)\n",
    "    output_images = sess.run(outputs, feed_dict={inputs: splited_images_})\n",
    "    \n",
    "    output_image = join_patches(output_images, n1, n2, [1, 128, 512,1], [1,64,256,1])\n",
    "    output_image = output_image[args.k_size[0]:-args.k_size[0],  #channel1\n",
    "                                args.k_size[1]:-args.k_size[1],  #channel2\n",
    "                                :]#channel 3\n",
    "    output_image = sess.run(output_image)\n",
    "    output_image = cv2.resize(output_image, (w, h))\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r output/run_method3\n",
    "args.output_dir = 'output/run_method3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sample: 151 data/ano\n",
      "data/ano/77.png\n",
      "output/run_method3/[64, 256]_77_input.png\n",
      "data/ano/63.png\n",
      "output/run_method3/[64, 256]_63_input.png\n",
      "data/ano/62.png\n",
      "output/run_method3/[64, 256]_62_input.png\n",
      "data/ano/76.png\n",
      "output/run_method3/[64, 256]_76_input.png\n",
      "data/ano/89.png\n",
      "output/run_method3/[64, 256]_89_input.png\n",
      "data/ano/149.png\n",
      "output/run_method3/[64, 256]_149_input.png\n",
      "data/ano/60.png\n",
      "output/run_method3/[64, 256]_60_input.png\n",
      "data/ano/74.png\n",
      "output/run_method3/[64, 256]_74_input.png\n",
      "data/ano/48.png\n",
      "output/run_method3/[64, 256]_48_input.png\n",
      "data/ano/49.png\n",
      "output/run_method3/[64, 256]_49_input.png\n",
      "data/ano/75.png\n",
      "output/run_method3/[64, 256]_75_input.png\n",
      "data/ano/61.png\n",
      "output/run_method3/[64, 256]_61_input.png\n",
      "data/ano/148.png\n",
      "output/run_method3/[64, 256]_148_input.png\n",
      "data/ano/59.png\n",
      "output/run_method3/[64, 256]_59_input.png\n",
      "data/ano/65.png\n",
      "output/run_method3/[64, 256]_65_input.png\n",
      "data/ano/70.png\n",
      "output/run_method3/[64, 256]_70_input.png\n",
      "data/ano/64.png\n",
      "output/run_method3/[64, 256]_64_input.png\n",
      "data/ano/58.png\n",
      "output/run_method3/[64, 256]_58_input.png\n",
      "data/ano/99.png\n",
      "output/run_method3/[64, 256]_99_input.png\n",
      "data/ano/8.png\n",
      "output/run_method3/[64, 256]_8_input.png\n",
      "data/ano/72.png\n",
      "output/run_method3/[64, 256]_72_input.png\n",
      "data/ano/66.png\n",
      "output/run_method3/[64, 256]_66_input.png\n",
      "data/ano/67.png\n",
      "output/run_method3/[64, 256]_67_input.png\n",
      "data/ano/73.png\n",
      "output/run_method3/[64, 256]_73_input.png\n",
      "data/ano/9.png\n",
      "output/run_method3/[64, 256]_9_input.png\n",
      "data/ano/98.png\n",
      "output/run_method3/[64, 256]_98_input.png\n",
      "data/ano/129.png\n",
      "output/run_method3/[64, 256]_129_input.png\n",
      "data/ano/115.png\n",
      "output/run_method3/[64, 256]_115_input.png\n",
      "data/ano/101.png\n",
      "output/run_method3/[64, 256]_101_input.png\n",
      "data/ano/14.png\n",
      "output/run_method3/[64, 256]_14_input.png\n",
      "data/ano/28.png\n",
      "output/run_method3/[64, 256]_28_input.png\n",
      "data/ano/29.png\n",
      "output/run_method3/[64, 256]_29_input.png\n",
      "data/ano/15.png\n",
      "output/run_method3/[64, 256]_15_input.png\n",
      "data/ano/100.png\n",
      "output/run_method3/[64, 256]_100_input.png\n",
      "data/ano/114.png\n",
      "output/run_method3/[64, 256]_114_input.png\n",
      "data/ano/128.png\n",
      "output/run_method3/[64, 256]_128_input.png\n",
      "data/ano/102.png\n",
      "output/run_method3/[64, 256]_102_input.png\n",
      "data/ano/116.png\n",
      "output/run_method3/[64, 256]_116_input.png\n",
      "data/ano/17.png\n",
      "output/run_method3/[64, 256]_17_input.png\n",
      "data/ano/16.png\n",
      "output/run_method3/[64, 256]_16_input.png\n",
      "data/ano/117.png\n",
      "output/run_method3/[64, 256]_117_input.png\n",
      "data/ano/103.png\n",
      "output/run_method3/[64, 256]_103_input.png\n",
      "data/ano/107.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-54d1eb93718b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmerge_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# os.makedirs('output/{}_{}'.format(args.stride, name), exist_ok=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "start = time()\n",
    "paths = glob('{}/*.png'.format(args.input_dir))\n",
    "paths = [path for path in paths]\n",
    "assert len(paths) > 0\n",
    "print('Num of sample:', len(paths), args.input_dir)\n",
    "inputs = get_tensor_by_name('inputs')\n",
    "outputs = get_tensor_by_name('outputs')\n",
    "\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    name = path.split('/')[-1].split('.')[0]\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    output_image = sess.run(outputs, {inputs:image})\n",
    "    merge_image = 0.5*image+0.5*output_image\n",
    "    # os.makedirs('output/{}_{}'.format(args.stride, name), exist_ok=True)\n",
    "    print('{}/{}_{}_input.png'.format(args.output_dir,args.strides, name))\n",
    "    cv2.imwrite('{}/{}_{}_input.png'.format(args.output_dir,args.strides, name), image)\n",
    "    cv2.imwrite('{}/{}_{}_output.png'.format(args.output_dir, args.strides, name), output_image)\n",
    "    cv2.imwrite('{}/{}_{}_merge.png'.format(args.output_dir, args.strides, name), merge_image)\n",
    "print('Running time:', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r output/run_method3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/run_method2'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "a = tf.placeholder(tf.float32, [None, 100, 100, 3])\n",
    "b = tf.layers.conv2d(a, 1, 3)\n",
    "row = tf.train.slice_input_producer([a], num_epochs=1, shuffle=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,100,100,3]\n",
      "\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,100,100,3], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
